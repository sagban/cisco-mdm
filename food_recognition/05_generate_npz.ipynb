{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05_generate_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(img_size, boxes):\n",
    "    \"\"\" Box preprocessing: based on two diagonal coordinates convert box info to boxcenter_x, boxcenter_y, w, h\n",
    "    and find the maximum number of boxes then do padding for all the boxes based on the maximum #boxes\n",
    "    :param boxes: array with pure box diagonal coordinates info from train_uec100.txt\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Original boxes stored as 1D list of class, x_min, y_min, x_max, y_max.\n",
    "    boxes = [box.reshape((-1, 5)) for box in boxes]\n",
    "\n",
    "    # Get box parameters as x_center, y_center, box_width, box_height, class.\n",
    "    boxes_xy = [0.5 * (box[:, 3:5] + box[:, 1:3]) for box in boxes]\n",
    "    boxes_wh = [box[:, 3:5] - box[:, 1:3] for box in boxes]\n",
    "    boxes_xy = [boxxy / img_size for boxxy in boxes_xy]\n",
    "    boxes_wh = [boxwh / img_size for boxwh in boxes_wh]\n",
    "    boxes = [np.concatenate((boxes_xy[i], boxes_wh[i], box[:, 0:1]), axis=1) for i, box in enumerate(boxes)]\n",
    "\n",
    "    # find the max number of boxes\n",
    "    max_boxes = 0\n",
    "    for box in boxes:\n",
    "        if box.shape[0] > max_boxes:\n",
    "            max_boxes = box.shape[0]\n",
    "\n",
    "    # add zero pad for training\n",
    "    for i, box in enumerate(boxes):\n",
    "        if box.shape[0] < max_boxes:\n",
    "            zero_padding = np.zeros((max_boxes - box.shape[0], 5), dtype=np.float32)\n",
    "            boxes[i] = np.vstack((box, zero_padding))\n",
    "\n",
    "    return np.array(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_dict(class_path):\n",
    "    print('\\n-> creating dictinary for labels...\\n')\n",
    "    label_dict = {}\n",
    "    with open(class_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    for i in range(0, len(class_names)):\n",
    "        label_dict[class_names[i][:-1]] = i\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt2data(txt_path):\n",
    "    \"\"\" Read train_uec100.txt file and convert to image_data array\n",
    "    :return: image_data array with ['Volumes/JS/UECFOOD100_JS/1/1.jpg', [0,0,143,370,486]] kind of entries\n",
    "    \"\"\"\n",
    "    print('\\n-> converting txt info to data...\\n')\n",
    "\n",
    "    # Read train_uec100.txt file and save to a dict with directory as the key, bbox as value\n",
    "    with open(txt_path, 'r') as f:\n",
    "        entries = f.readlines()\n",
    "        out = {}\n",
    "        for i, entry in enumerate(entries):\n",
    "            if i > 0:  # skip header\n",
    "                print(entry)\n",
    "                entry = entry[:-1].split(' ')\n",
    "                assert Image.open(entry[0]).size == (224, 224)  # after preprocessing size should be exactly 800,600\n",
    "                entry[2] = ' '.join(entry[2:])\n",
    "                entry[1] = str(int(entry[1]) - 1) + ' ' + entry[2]  # YOLO requires category id starts from 0 not 1\n",
    "                entry = entry[:2]\n",
    "                if entry[0] in out.keys():\n",
    "                    out[entry[0]].append(entry[1])\n",
    "                else:\n",
    "                    out[entry[0]] = [entry[1]]\n",
    "\n",
    "    # Save img directory with bbox info from out dict to image_data array\n",
    "    image_data = list()\n",
    "    index = 0\n",
    "    for k, v in out.items():\n",
    "        image_data.append([k])\n",
    "        for i in v:\n",
    "            image_data[index].append(i)\n",
    "        index += 1\n",
    "\n",
    "    # Convert string to int or float and save in image_data array again\n",
    "    for no, entry in enumerate(image_data):\n",
    "        for i, box in enumerate(entry):\n",
    "            if i != 0:      # skip img path\n",
    "                box = box.split(' ')\n",
    "                box[0] = int(box[0])  # convert class name to numbers (0~)\n",
    "\n",
    "                for k in range(1, 5):  # Change box boundaries from str to int\n",
    "                    box[k] = int(float(box[k]))\n",
    "\n",
    "                image_data[no][i] = box\n",
    "    return image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(image_data):\n",
    "    \"\"\" Load images based on their directory in the image_data array and save them in images then return\n",
    "    :param image_data: acquired from txt2data()\n",
    "    :return: images with each img info with shape(600, 800, 3) for each\n",
    "    \"\"\"\n",
    "    print('\\n -> Reading imgs and saving to array images...\\n')\n",
    "    images = []\n",
    "    boxes = np.array([np.array(image_data[i][1:]) for i in range(np.array(image_data).shape[0])])\n",
    "    image_data = np.array(image_data)\n",
    "    boxes = process_data(img_size, boxes)\n",
    "    detectors_mask, matching_true_boxes = get_detector_mask(boxes, anchors)\n",
    "\n",
    "    for i, data in enumerate(image_data):\n",
    "        img = Image.open(os.path.join(data[0]))\n",
    "        assert img.size == (800, 600)\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        images.append(img)\n",
    "        boxes = np.array(image_data[i][1:])\n",
    "        boxes = np.array(boxes)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def images2npv(images, image_data, shuffle=False):\n",
    "    \"\"\" Save image info and box info to npv file\n",
    "    :param images: image\n",
    "    :param image_data:\n",
    "    :param shuffle: if shuffle or not (data has been shuffled in during preprocessing)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print('\\n -> converting image info to npv file...\\n')\n",
    "    images = np.array(images, dtype=np.uint8)\n",
    "    image_data = [np.array(image_data[i][1:]) for i in range(images.shape[0])]\n",
    "    image_data = np.array(image_data)\n",
    "\n",
    "    # shuffle dataset\n",
    "    if shuffle:\n",
    "        np.random.seed(13)\n",
    "        indices = np.arange(len(images))\n",
    "        np.random.shuffle(indices)\n",
    "        images, image_data = images[indices], image_data[indices]\n",
    "    print('dataset contains {} images'.format(images.shape[0]))\n",
    "    np.savez('FoodDataset_npv', image=images, boxes=image_data)\n",
    "    print('npz file has been generated and saved as FoodDataset_npv.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detector_mask(boxes, anchors):\n",
    "    detectors_mask = [0 for i in range(len(boxes))]\n",
    "    matching_true_boxes = [0 for i in range(len(boxes))]\n",
    "    for i, box in enumerate(boxes):\n",
    "        detectors_mask[i], matching_true_boxes[i] = preprocess_true_boxes(box, anchors, [416, 416])\n",
    "\n",
    "    return np.array(detectors_mask), np.array(matching_true_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_true_boxes(true_boxes, anchors, image_size):\n",
    "    \"\"\"Find detector in YOLO where ground truth box should appear\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    true_boxes : array\n",
    "        List of ground truth boxes in form of relative x, y, w, h, class.\n",
    "        Relative coordinates are in the range [0, 1] indicating a percentage\n",
    "        of the original image dimensions.\n",
    "    anchors : array\n",
    "        List of anchors in form of w, h.\n",
    "        Anchors are assumed to be in the range [0, conv_size] where conv_size\n",
    "        is the spatial dimension of the final convolutional features.\n",
    "    image_size : array-like\n",
    "        List of image dimensions in form of h, w in pixels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    detectors_mask : array\n",
    "        0/1 mask for detectors in [conv_height, conv_width, num_anchors, 1]\n",
    "        that should be compared with a matching ground truth box.\n",
    "    matching_true_boxes: array\n",
    "        Same shape as detectors_mask with the corresponding ground truth box\n",
    "        adjusted for comparison with predicted parameters at training time.\n",
    "    \"\"\"\n",
    "    height, width = image_size\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    # Downsampling factor of 5x 2-stride max_pools == 32.\n",
    "    assert height % 32 == 0,    'Image sizes in YOLO_v2 must be multiples of 32.'\n",
    "    assert width % 32 == 0,     'Image sizes in YOLO_v2 must be multiples of 32.'\n",
    "    conv_height = height // 32\n",
    "    conv_width = width // 32\n",
    "    num_box_params = true_boxes.shape[1]\n",
    "    detectors_mask = np.zeros(\n",
    "        (conv_height, conv_width, num_anchors, 1), dtype=np.float32)\n",
    "    matching_true_boxes = np.zeros(\n",
    "        (conv_height, conv_width, num_anchors, num_box_params),\n",
    "        dtype=np.float32)\n",
    "\n",
    "    for box in true_boxes:\n",
    "        # scale box to convolutional feature spatial dimensions\n",
    "        box_class = box[4:5]\n",
    "        box = box[0:4] * np.array(\n",
    "            [conv_width, conv_height, conv_width, conv_height])\n",
    "        i = np.floor(box[1]).astype('int')\n",
    "        j = np.floor(box[0]).astype('int')\n",
    "        if j >= 13 or i >= 13:\n",
    "            print('bug')\n",
    "        best_iou = 0\n",
    "        best_anchor = 0\n",
    "        for k, anchor in enumerate(anchors):\n",
    "            # Find IOU between box shifted to origin and anchor box.\n",
    "            box_maxes = box[2:4] / 2.\n",
    "            box_mins = -box_maxes\n",
    "            anchor_maxes = (anchor / 2.)\n",
    "            anchor_mins = -anchor_maxes\n",
    "\n",
    "            intersect_mins = np.maximum(box_mins, anchor_mins)\n",
    "            intersect_maxes = np.minimum(box_maxes, anchor_maxes)\n",
    "            intersect_wh = np.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "            intersect_area = intersect_wh[0] * intersect_wh[1]\n",
    "            box_area = box[2] * box[3]\n",
    "            anchor_area = anchor[0] * anchor[1]\n",
    "            iou = intersect_area / (box_area + anchor_area - intersect_area)\n",
    "            if iou > best_iou:\n",
    "                best_iou = iou\n",
    "                best_anchor = k\n",
    "\n",
    "        if best_iou > 0:\n",
    "            print(i, j, best_anchor)\n",
    "            detectors_mask[i, j, best_anchor] = 1\n",
    "            adjusted_box = np.array(\n",
    "                [\n",
    "                    box[0] - j, box[1] - i,\n",
    "                    np.log(box[2] / anchors[best_anchor][0]),\n",
    "                    np.log(box[3] / anchors[best_anchor][1]), box_class\n",
    "                ],\n",
    "                dtype=np.float32)\n",
    "            matching_true_boxes[i, j, best_anchor] = adjusted_box\n",
    "    return detectors_mask, matching_true_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-32-b0b594c8d06e>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-32-b0b594c8d06e>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    with open(\"Dataset\", generated_anchors_mobilenet\\\\anchors_10.txt\", 'r') as anchor_file:\u001b[0m\n\u001b[1;37m                                                                                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# Read anchor_10.txt\n",
    "anchors = []\n",
    "with open(os.path.join(\"Dataset\", \"generated_anchors_mobilenet\", \"anchors_10.txt\"), 'r') as anchor_file:\n",
    "    for i, line in enumerate(anchor_file):\n",
    "        line = line.rstrip('\\n')\n",
    "        anchors.append(list(map(float, line.split(', '))))\n",
    "anchors = np.array(anchors)\n",
    "print('-> anchors acquired\\n')\n",
    "print(anchors)\n",
    "\n",
    "txt_path = os.path.join(\"Dataset\", \"train_food.txt\")\n",
    "img_size = np.array([224, 224])\n",
    "\n",
    "# Generate dictionary with labels and ids (not necessary)\n",
    "label_dict = create_label_dict(os.path.join(\"Dataset\", \"classes.txt\"))\n",
    "print(label_dict)\n",
    "\n",
    "# Convert txt info to data\n",
    "image_data = txt2data(txt_path)\n",
    "images = load_images(image_data)\n",
    "images2npv(images, image_data)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
